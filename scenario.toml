# AgentJustice Leaderboard - Assessment Configuration
#
# Green Agent: AgentJustice Evaluator
#   - Orchestrates evaluation using MCP-based judges
#   - Computes hybrid scores with semantic, numeric, and contradiction analysis
#
# Purple Agent: Financial Answering Agent (submitted by participants)
#   - Generates answers to financial questions from FinanceBench dataset

[green_agent]
agentbeats_id = "019c15cc-a1e3-7481-8c68-8d7ad1045bd2"

[green_agent.env]
# OpenAI API key for MCP judges (semantic, numeric, contradiction)
OPENAI_API_KEY = "${OPENAI_API_KEY}"

# Purple Agent - Answer Generator
# Submitters: Fill in your purple agent's agentbeats_id below
[[participants]]
name = "purple_agent"
agentbeats_id = ""  # <-- Submitters: Add your purple agent ID here

[participants.env]
# Submitters: Add any environment variables your purple agent needs
OPENAI_API_KEY = "${OPENAI_API_KEY}"

# Assessment Configuration
[config]
# Dataset to evaluate: "canonical" (50 tasks) or "adversarial"
dataset = "canonical"

# Purple agent mode: "llm" (generate answers) or "gold" (return gold answers for testing)
purple_agent_mode = "llm"

# Number of tasks to evaluate (0 = all tasks in dataset)
task_limit = 0

# Numeric tolerance for scoring (1% = 0.01)
numeric_tolerance = 0.01
